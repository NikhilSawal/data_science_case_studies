{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employer Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_title(data, pattern, category_name):\n",
    "    \"\"\"\n",
    "    Find patterns and return a list with the common category name for \n",
    "    different patterns. For eg: If 'Walmart' appears in the following \n",
    "    formats ('walmart', 'Wal-Mart', 'Walmart')\n",
    "    \"\"\"\n",
    "    unique_names = data['emp_title'].unique() \n",
    "    matches = [pattern.findall(i) for i in unique_names if len(pattern.findall(i)) > 0]\n",
    "    matches = [item for l in matches for item in l]\n",
    "    return [category_name if i in matches else i for i in data['emp_title']]\n",
    "\n",
    "\n",
    "def emp_title_patterns(data, col_name):\n",
    "    \"\"\"\n",
    "    Identify and apply patterns of most common employer title and \n",
    "    replace the remaining with Others!!\n",
    "    \"\"\"\n",
    "    \n",
    "    # US Army\n",
    "    pattern = re.compile(r'[a-zA-Z\\S]*^[uU][a-zA-Z\\s\\S]+[Aa][Rr][Mm][Yy][a-zA-Z\\S]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'U.S. Army')\n",
    "\n",
    "    # US Navy\n",
    "    pattern = re.compile(r'^[uU][a-zA-Z\\s\\S]+[Nn][Aa][Vv][Yy][a-zA-Z\\S]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'U.S. Navy')\n",
    "\n",
    "    # Walmart\n",
    "    pattern = re.compile(r'^[Ww][Aa][a-zA-Z\\S]+[tT]$[a-zA-Z\\S]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'Walmart')\n",
    "\n",
    "    # Banks\n",
    "    pattern = re.compile(r'[a-zA-Z\\S\\s]*[Bb][Aa][Nn][Kk][a-zA-Z\\S\\s]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'Banks')\n",
    "\n",
    "    # AT&T\n",
    "    pattern = re.compile(r'[a-zA-Z\\S\\s]*[Aa][Tt][&n][Tt][a-zA-Z\\S\\s]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'AT&T')\n",
    "\n",
    "    # Air Force\n",
    "    pattern = re.compile(r'[a-zA-Z\\S\\s]*Force[a-zA-Z\\S\\s]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'Air Force')\n",
    "\n",
    "    # USPS\n",
    "    pattern = re.compile(r'^[Uu][a-zA-Z\\S\\s]*[Pp][Oo][Ss][Tt][Aa][Ll][a-zA-Z\\S\\s]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'USPS')\n",
    "\n",
    "    # USPS\n",
    "    pattern = re.compile(r'^[Uu][Ss][Pp][Ss]')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'USPS')\n",
    "\n",
    "    # Chase Bank\n",
    "    pattern = re.compile(r'[a-zA-Z\\S\\s]*[Cc]hase[a-zA-Z\\S\\s]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'J.P. Morgan Chase')\n",
    "\n",
    "    # IBM\n",
    "    pattern = re.compile(r'[a-zA-Z\\S\\s]*[Ii][Bb][Mm][a-zA-Z\\S\\s]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'IBM')\n",
    "\n",
    "    # University\n",
    "    pattern = re.compile(r'[a-zA-Z\\S\\s]*[Uu][Nn][Ii][Vv][Ee][Rr][Ss][Ii][Tt][Yy][a-zA-Z\\S\\s]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'University')\n",
    "\n",
    "    # Airlines\n",
    "    pattern = re.compile(r'[a-zA-Z\\S\\s]*[Aa]irline[s]*[a-zA-Z\\S\\s]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'Airlines')\n",
    "\n",
    "    # The Home Depot\n",
    "    pattern = re.compile(r'[a-zA-Z\\S\\s]*[Hh][Oo][Mm][Ee]\\s[Dd][Ee][Pp][Oo][Tt][a-zA-Z\\S\\s]*')\n",
    "    data.loc[:,col_name] = emp_title(data, pattern, 'The Home Depot')\n",
    "\n",
    "    # Other\n",
    "    top_category = ['None', 'Banks', 'University', 'U.S. Army', 'Air Force', 'USPS', 'Airlines', 'Walmart', 'J.P. Morgan Chase', 'IBM', 'U.S. Navy', 'The Home Depot', 'AT&T']\n",
    "    data.loc[:,col_name] = ['Other' if i not in top_category else i for i in data.loc[:,col_name]] \n",
    "    \n",
    "    return data[col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_nltk_notes(data, col_names):\n",
    "\n",
    "    data.loc[:, col_names] = data[col_names].fillna('None')\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.append('br/')\n",
    "    special_char = re.compile(r'[\\W]')\n",
    "    ps = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    sample = []\n",
    "    for index, note in enumerate(data[col_names]):\n",
    "        word_tokens = word_tokenize(note)\n",
    "        no_stops = [i for i in word_tokens if i.lower() not in stop_words]\n",
    "        no_special = [special_char.sub('',i) for i in no_stops if special_char.sub('',i) != '']\n",
    "        stem_lemma = \" \".join(ps.stem(lemmatizer.lemmatize(i.lower())) for i in no_special)\n",
    "        sample.append(stem_lemma)\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessiong(data):\n",
    "    \n",
    "    # emp_title - CATEGORICAL\n",
    "    data.loc[:,'emp_title'] = data['emp_title'].fillna('None')\n",
    "    data.loc[:,'emp_title'] = emp_title_patterns(data, 'emp_title')\n",
    "    \n",
    "    # emp_length - NUMERICAL\n",
    "    data.loc[:,'emp_length'] = [0 if i == 'na' else i for i in data['emp_length']]\n",
    "    data.loc[:,'emp_length'] = data['emp_length'].astype(int)\n",
    "    \n",
    "    # home_ownership - CATEGORICAL\n",
    "    data.drop(data.loc[data['home_ownership']=='NONE', :].index, inplace=True)\n",
    "    \n",
    "    # annual_inc - NUMERICAL\n",
    "    data.loc[:,'annual_inc'] = data['annual_inc'].fillna(0)\n",
    "    \n",
    "    # verification_status - CATEGORICAL\n",
    "    \n",
    "    # Notes - TEXT\n",
    "    data.loc[:,'Notes'] = lambda_nltk_notes(data, 'Notes')\n",
    "    data.loc[:,'Notes'] = data['Notes'].astype(str)\n",
    "    \n",
    "    # purpose_cat - CATEGORICAL\n",
    "    purpose_df = pd.DataFrame(data['purpose_cat'].value_counts())\n",
    "    purpose_df.reset_index(inplace=True) \n",
    "    other_small_business = list(purpose_df.loc[purpose_df['purpose_cat'] < 90, 'index'])\n",
    "    data.loc[:,'purpose_cat'] = [i if i not in other_small_business else 'other small business' for i in data['purpose_cat']]\n",
    "    \n",
    "    # addr_state - CATEGORICAL\n",
    "    \n",
    "    # debt_to_income - NUMERICAL\n",
    "    \n",
    "    # delinq_2yrs - NUMERICAL\n",
    "    data.loc[:,'delinq_2yrs'] = data['delinq_2yrs'].fillna(0.0)\n",
    "    \n",
    "    # earliest_cr_line - CATEGORICAL\n",
    "    data.loc[:,'quarter'] = [str(i.quarter) for i in pd.to_datetime(data['earliest_cr_line'])]\n",
    "    data.loc[:,'year'] = [str(i.year) for i in pd.to_datetime(data['earliest_cr_line'])]\n",
    "    \n",
    "    #### replace less frequent with 'Other' \n",
    "    cr_line_df = pd.DataFrame(data['year'].value_counts())\n",
    "    cr_line_df.reset_index(inplace=True) \n",
    "    other_years = list(cr_line_df.loc[cr_line_df['year'] < 90, 'index'])\n",
    "    data.loc[:,'year'] = [i if i not in other_years else 'other' for i in data['year']]\n",
    "\n",
    "    # inq_last_6mths - NUMERICAL\n",
    "    data.loc[:,'inq_last_6mths'] = data['inq_last_6mths'].fillna(0)\n",
    "    \n",
    "    # mths_since_last_delinq - NUMERICAL\n",
    "    data.loc[:,'mths_since_last_delinq'] = data['mths_since_last_delinq'].fillna(0)\n",
    "    \n",
    "    # mths_since_last_record - NUMERICAL\n",
    "    data.loc[:,'mths_since_last_record'] = data['mths_since_last_record'].fillna(0)\n",
    "    \n",
    "    # open_acc - NUMERICAL\n",
    "    data.loc[:,'open_acc'] = data['open_acc'].fillna(data['open_acc'].mean())\n",
    "    \n",
    "    # pub_rec - NUMERICAL\n",
    "    data.loc[:,'pub_rec'] = data['pub_rec'].fillna(0.0)\n",
    "    \n",
    "    # revol_bal - NUMERICAL\n",
    "    \n",
    "    # revol_util - NUMERICAL\n",
    "    data.loc[:,'revol_util'] = data['revol_util'].fillna(data['revol_util'].mean())\n",
    "    \n",
    "    # total_acc - NUMERICAL\n",
    "    data.loc[:,'total_acc'] = data['total_acc'].fillna(data['total_acc'].mean())\n",
    "    \n",
    "    # mths_since_last_major_derog - NUMERICAL\n",
    "    \n",
    "    # policy_code - CATEGORICAL\n",
    "    \n",
    "    # Drop columns\n",
    "    data.drop(columns=['Id', 'pymnt_plan', 'purpose', 'initial_list_status', 'collections_12_mths_ex_med', 'earliest_cr_line', 'zip_code'], axis=1, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:,~df.columns.isin(['is_bad'])].copy()\n",
    "y = df['is_bad']\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y,\n",
    "                                        test_size=0.20,\n",
    "                                        random_state=1,\n",
    "                                        stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size=0.20,\n",
    "                                                  random_state=1,\n",
    "                                                  stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 27) (1600, 27) (2000, 27)\n",
      "(6400,) (1600,) (2000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.drop(X_train[X_train['home_ownership']=='NONE'].index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_preprocessiong(X_train.copy())\n",
    "X_val = data_preprocessiong(X_val.copy())\n",
    "X_test = data_preprocessiong(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['emp_title', 'emp_length', 'home_ownership', 'verification_status',\n",
       "       'pymnt_plan', 'Notes', 'purpose_cat', 'purpose', 'zip_code',\n",
       "       'addr_state', 'earliest_cr_line', 'initial_list_status', 'policy_code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'is_bad', 'annual_inc', 'debt_to_income', 'delinq_2yrs',\n",
       "       'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record',\n",
       "       'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n",
       "       'collections_12_mths_ex_med', 'mths_since_last_major_derog'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore') \n",
    "cat_x_train = ohe.fit_transform(X_train[['emp_title', 'home_ownership', 'verification_status', \n",
    "                                         'purpose_cat', 'addr_state', 'policy_code', 'quarter', 'year']])\n",
    "\n",
    "mmscaler = MinMaxScaler(feature_range=(0,1))\n",
    "num_x_train = mmscaler.fit_transform(X_train[['emp_length', 'annual_inc', 'debt_to_income', 'delinq_2yrs',\n",
    "                                'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', \n",
    "                                'revol_util', 'total_acc', 'mths_since_last_major_derog', \n",
    "                                'mths_since_last_record', 'mths_since_last_delinq']])\n",
    "\n",
    "tf = TfidfVectorizer(min_df=1, stop_words='english', lowercase=True)\n",
    "text_x_train = tf.fit_transform(X_train['Notes']).toarray()\n",
    "\n",
    "X_train = np.concatenate((cat_x_train, num_x_train, text_x_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6399, 112) (6399, 13) (6399, 9760)\n"
     ]
    }
   ],
   "source": [
    "print(cat_x_train.shape, num_x_train.shape, text_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 9885)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_x_val = ohe.transform(X_val[['emp_title', 'home_ownership', 'verification_status', \n",
    "                                 'purpose_cat', 'addr_state', 'policy_code', 'quarter', 'year']])\n",
    "\n",
    "num_x_val = mmscaler.transform(X_val[['emp_length', 'annual_inc', 'debt_to_income', 'delinq_2yrs',\n",
    "                                'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', \n",
    "                                'revol_util', 'total_acc', 'mths_since_last_major_derog', \n",
    "                                'mths_since_last_record', 'mths_since_last_delinq']])\n",
    "\n",
    "text_x_val = tf.transform(X_val['Notes']).toarray()\n",
    "\n",
    "X_val = np.concatenate((cat_x_val, num_x_val, text_x_val), axis=1)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 9885)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_x_test = ohe.transform(X_test[['emp_title', 'home_ownership', 'verification_status', \n",
    "                                   'purpose_cat', 'addr_state', 'policy_code', 'quarter', 'year']])\n",
    "\n",
    "num_x_test = mmscaler.transform(X_test[['emp_length', 'annual_inc', 'debt_to_income', 'delinq_2yrs',\n",
    "                                'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', \n",
    "                                'revol_util', 'total_acc', 'mths_since_last_major_derog', \n",
    "                                'mths_since_last_record', 'mths_since_last_delinq']])\n",
    "\n",
    "text_x_test = tf.transform(X_test['Notes']).toarray()\n",
    "\n",
    "X_test = np.concatenate((cat_x_test, num_x_test, text_x_test), axis=1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-aucpr:0.18182\tvalidation_1-aucpr:0.14960\n",
      "[1]\tvalidation_0-aucpr:0.21049\tvalidation_1-aucpr:0.15041\n",
      "[2]\tvalidation_0-aucpr:0.22096\tvalidation_1-aucpr:0.15675\n",
      "[3]\tvalidation_0-aucpr:0.29287\tvalidation_1-aucpr:0.17825\n",
      "[4]\tvalidation_0-aucpr:0.29859\tvalidation_1-aucpr:0.17883\n",
      "[5]\tvalidation_0-aucpr:0.32106\tvalidation_1-aucpr:0.18272\n",
      "[6]\tvalidation_0-aucpr:0.32411\tvalidation_1-aucpr:0.18416\n",
      "[7]\tvalidation_0-aucpr:0.33225\tvalidation_1-aucpr:0.18440\n",
      "[8]\tvalidation_0-aucpr:0.33382\tvalidation_1-aucpr:0.18623\n",
      "[9]\tvalidation_0-aucpr:0.33042\tvalidation_1-aucpr:0.18630\n",
      "[10]\tvalidation_0-aucpr:0.33615\tvalidation_1-aucpr:0.18684\n",
      "[11]\tvalidation_0-aucpr:0.33944\tvalidation_1-aucpr:0.18718\n",
      "[12]\tvalidation_0-aucpr:0.33965\tvalidation_1-aucpr:0.18697\n",
      "[13]\tvalidation_0-aucpr:0.34309\tvalidation_1-aucpr:0.18909\n",
      "[14]\tvalidation_0-aucpr:0.34513\tvalidation_1-aucpr:0.18924\n",
      "[15]\tvalidation_0-aucpr:0.34497\tvalidation_1-aucpr:0.18955\n",
      "[16]\tvalidation_0-aucpr:0.34883\tvalidation_1-aucpr:0.19124\n",
      "[17]\tvalidation_0-aucpr:0.35283\tvalidation_1-aucpr:0.19129\n",
      "[18]\tvalidation_0-aucpr:0.35310\tvalidation_1-aucpr:0.19087\n",
      "[19]\tvalidation_0-aucpr:0.35493\tvalidation_1-aucpr:0.19035\n",
      "[20]\tvalidation_0-aucpr:0.35441\tvalidation_1-aucpr:0.19010\n"
     ]
    }
   ],
   "source": [
    "optimized_classifier = XGBClassifier(objective='binary:logistic',\n",
    "                                     gamma=10.0,\n",
    "                                     learning_rate=0.1,\n",
    "                                     max_depth=3,\n",
    "                                     reg_lambda=100.0,\n",
    "                                     scale_pos_weight=7,\n",
    "                                     use_label_encoder=False,\n",
    "                                     subsample=0.95,\n",
    "                                     colsample_bytree=0.5,\n",
    "                                     seed=10)\n",
    "\n",
    "model = optimized_classifier.fit(X_train,\n",
    "                                 y_train,\n",
    "                                 early_stopping_rounds=10,\n",
    "                                 verbose=True,\n",
    "                                 eval_metric='aucpr',\n",
    "                                 eval_set=[(X_train, y_train), (X_val, y_val)])\n",
    "\n",
    "plot_confusion_matrix(optimized_classifier,\n",
    "                      X_val,\n",
    "                      y_val,\n",
    "                      values_format='d',\n",
    "                      display_labels=[\"No Default\",\n",
    "                                      \"Default\"])\n",
    "\n",
    "plot_confusion_matrix(optimized_classifier,\n",
    "                      X_test,\n",
    "                      y_test,\n",
    "                      values_format='d',\n",
    "                      display_labels=[\"No Default\",\n",
    "                                      \"Default\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation - Test-Validation Set\n",
    "print('Validation Set Metrics: ')\n",
    "print('-----------------------')\n",
    "y_pred_val = optimized_classifier.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "print('Confusion matrix: \\n', cm)\n",
    "print('\\nF1 Score: ', 2*cm[1, 1]/(2*cm[1, 1]+cm[0, 1]+cm[1, 0]))\n",
    "print('\\nPrecision: ', cm[1, 1]/(cm[1, 1]+cm[0, 1]))\n",
    "print('\\nAccuracy: ', accuracy)\n",
    "print('\\nRecall/Sensitivity: ', cm[1, 1]/(cm[1, 1]+cm[1, 0]))\n",
    "print('\\nSpecificity: ', cm[0, 0]/(cm[0, 0]+cm[0, 1]))\n",
    "\n",
    "print('\\n')\n",
    "print('Test Set Metrics: ')\n",
    "print('-----------------')\n",
    "y_pred_test = optimized_classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print('Confusion matrix: \\n', cm)\n",
    "print('\\nF1 Score: ', 2*cm[1, 1]/(2*cm[1, 1]+cm[0, 1]+cm[1, 0]))\n",
    "print('\\nPrecision: ', cm[1, 1]/(cm[1, 1]+cm[0, 1]))\n",
    "print('\\nAccuracy: ', accuracy)\n",
    "print('\\nRecall/Sensitivity: ', cm[1, 1]/(cm[1, 1]+cm[1, 0]))\n",
    "print('\\nSpecificity: ', cm[0, 0]/(cm[0, 0]+cm[0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
