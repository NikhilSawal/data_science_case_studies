{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "from statistics import median, mean\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "pd.set_option('display.max_rows', 90000)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/nikhilsawal/OneDrive/machine_learning/data_science_case_studies/buildzoom/data/'\n",
    "\n",
    "train_x = pd.read_table(path + 'train_data.csv')\n",
    "test_x = pd.read_table(path + 'xtest_data.csv')\n",
    "test_y = pd.read_csv(path + 'ytest_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_uniques(df, col_name):\n",
    "    \n",
    "    \"\"\"Any instance of the license type may contain duplicate!! For example:\n",
    "    the GENERAL CONTRACTOR LICENSE may appear twice, but in reality doesn't \n",
    "    add any value to our model and need to be removed.\"\"\"\n",
    "    \n",
    "    uniques = []\n",
    "    for i in df[col_name]:\n",
    "        uniques += i\n",
    "    return list(set(uniques))\n",
    "\n",
    "\n",
    "def get_pattern(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function identifies all the different possible appearances \n",
    "    for a license type. For example: GENERAL CONTRACTOR LICENSE has the \n",
    "    following appearances:\n",
    "    >>> ['GENERAL CO', 'GENERAL C', 'GENERA',\n",
    "        'GENERAL CONTRACTOR LICENSE', 'GENERAL ', 'GENERAL CONT', \n",
    "        'GENERAL CONTRA', 'GENERAL']\n",
    "    \"\"\"\n",
    "    \n",
    "    uniques = get_uniques(df, 'licensetype')\n",
    "    unique = []\n",
    "    for i in range(len(uniques)):\n",
    "        \n",
    "        pattern = re.compile('^'+uniques[i][:5]+'*')\n",
    "        matches = []\n",
    "        for index, license in enumerate(uniques):\n",
    "\n",
    "            if pattern.search(license) is not None:\n",
    "                matches.append(license)\n",
    "        \n",
    "        if matches not in unique:\n",
    "            unique.append(matches)\n",
    "        pass\n",
    "    \n",
    "    licenseList = []\n",
    "    licenseDict = {}\n",
    "    \n",
    "    for licenses in unique:\n",
    "        licenseList.append(licenses)\n",
    "        licenseDict[max(licenses)] = licenses\n",
    "    \n",
    "    return licenseList\n",
    "  \n",
    "\n",
    "# Clean licensetype\n",
    "def clean_license(inp_list, pattern):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a list of patterns generated by the get_pattern()\n",
    "    function and replaces any unusual license type by the more general!!!\n",
    "    For example: ['GENERAL CO', 'GENERAL C', 'GENERA',\n",
    "                  'GENERAL CONTRACTOR LICENSE', 'GENERAL ', \n",
    "                  'GENERAL CONT', 'GENERAL CONTRA', 'GENERAL']\n",
    "        \n",
    "    will be replaces with 'GENERAL CONTRACTOR LICENSE'\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_list = []\n",
    "    for i in inp_list:\n",
    "        if i == 'None':\n",
    "            temp_list.append(i)\n",
    "        else:\n",
    "            for j in pattern:\n",
    "                temp = []\n",
    "                if i in j:\n",
    "                    temp_list.append(max(j).lower().replace(\" \", \"_\"))\n",
    "                    break\n",
    "    return temp_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Pattern:  0.21043539200000083\n",
      "Clean License:  0.17293184199999878\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "train_x.loc[:,'licensetype'] = train_x.loc[:,'licensetype'].fillna('None')\n",
    "train_x.loc[:,'licensetype'] = train_x.loc[:,'licensetype'].apply(lambda x: x.split(', ')).apply(lambda x: list(set(x)))\n",
    "pattern = get_pattern(train_x)\n",
    "stop = timeit.default_timer()\n",
    "print('Get Pattern: ', stop - start)\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "cleaned_license = [clean_license(item, pattern) for item in train_x['licensetype']]\n",
    "cleaned_license = ['-'.join(sorted(i)) for i in cleaned_license]\n",
    "train_x.loc[:,'licensetype'] = cleaned_license\n",
    "stop = timeit.default_timer()\n",
    "print('Clean License: ', stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## businessname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_businessname(data, n):\n",
    "    \"\"\"Set top N businessnames as factor\"\"\"\n",
    "    temp = data['businessname'].value_counts().head(n).index.values\n",
    "    top_n = [i.lower().replace(\" \",\"_\") if i in temp else 'Other' for i in data['businessname']]\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Name:  0.5888640219999992\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "train_x['businessname'].fillna('None', inplace=True)\n",
    "train_x.loc[:,'businessname'] = get_businessname(train_x, 100)\n",
    "stop = timeit.default_timer()\n",
    "print('Business Name: ', stop - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords, non alphabetic characters\n",
    "\n",
    "def nltk_description(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in text data in the form of description\n",
    "    which is first tokenized and later cleaned by removing all stopwords,\n",
    "    removing special characters, stemming each character to its root form\n",
    "    and returns a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "    num_pattern = re.compile(r'\\s*[\\W0-9\\s]\\s*')\n",
    "\n",
    "    sample = []\n",
    "\n",
    "    for index, description in enumerate(data[\"description\"]):\n",
    "        \n",
    "        words = word_tokenize(description)\n",
    "        no_stops = [i for i in words if i.lower() not in stop_words]\n",
    "        no_special_char = [ps.stem(num_pattern.sub(\"\",i)) for i in no_stops if ps.stem(num_pattern.sub(\"\",i)) != '']\n",
    "        descrip = \" \".join(i for i in no_special_char)\n",
    "        sample.append(descrip)\n",
    "    \n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:  45.767612867000004\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "train_x.loc[:,'description'] = train_x.loc[:,'description'].fillna('None')\n",
    "train_x.loc[:,'description'] = nltk_description(train_x)\n",
    "stop = timeit.default_timer()\n",
    "print('Description: ', stop - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_subtype(data):\n",
    "    \"\"\"Encode subtype using OneHotEncoding\"\"\"\n",
    "    data.loc[:,'subtype'] = data.loc[:,'subtype'].fillna('None')\n",
    "    z = data.loc[:,['subtype']].values\n",
    "    y = OneHotEncoder().fit_transform(z).toarray()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtype:  0.04627532799999301\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "train_x['subtype'] = train_x['subtype'].fillna('None')\n",
    "train_x['subtype'] = [i.lower().replace(\" \", \"_\") for i in train_x['subtype']]\n",
    "stop = timeit.default_timer()\n",
    "print('Subtype: ', stop - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "\n",
    "    # license type \n",
    "    data.loc[:,'licensetype'] = data.loc[:,'licensetype'].fillna('None')\n",
    "    data.loc[:,'licensetype'] = data.loc[:,'licensetype'].apply(lambda x: x.split(', ')).apply(lambda x: list(set(x)))\n",
    "      \n",
    "    pattern = get_pattern(data)\n",
    "    cleaned_license = [clean_license(item, pattern) for item in data['licensetype']]\n",
    "    cleaned_license = ['-'.join(sorted(i)) for i in cleaned_license]\n",
    "    data.loc[:,'licensetype'] = cleaned_license\n",
    "    \n",
    "    # Set top business names as factors\n",
    "    data['businessname'].fillna('None', inplace=True)\n",
    "    data.loc[:,'businessname'] = get_businessname(data, 100)\n",
    "    \n",
    "    # Set binary value for legal description\n",
    "    data.loc[:,'legaldescription'] = data['legaldescription'].fillna('None')\n",
    "    data.loc[:,'has_ld'] = [1 if i!='None' else 0 for i in data['legaldescription']]\n",
    "    \n",
    "    # tfidf for description\n",
    "    data.loc[:,'description'] = data.loc[:,'description'].fillna('None')\n",
    "    data.loc[:,'description'] = nltk_description(data)\n",
    "    \n",
    "    # Subtype\n",
    "    data['subtype'] = data['subtype'].fillna('None')\n",
    "    data['subtype'] = [i.lower().replace(\" \", \"_\") for i in data['subtype']]\n",
    "\n",
    "    # Job Value\n",
    "    cleaned_job_value = data['job_value'].apply(lambda x: float(str(x).replace('$', '').replace(',','')))\n",
    "    data.loc[:,'job_value'] = cleaned_job_value\n",
    "    data.loc[:,'job_value'] = data['job_value'].fillna(0.0)\n",
    "    \n",
    "    return data.loc[:, ~data.columns.isin(['legaldescription'])]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing:  40.470503638\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "data_preprocessing(train_x)\n",
    "stop = timeit.default_timer()\n",
    "print('Data preprocessing: ', stop - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Test data prep:  60.303732585999995\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "del(train_x)\n",
    "del(test_x)\n",
    "del(test_y)\n",
    "\n",
    "train_x = pd.read_table(path + 'train_data.csv')\n",
    "test_x = pd.read_table(path + 'xtest_data.csv')\n",
    "test_y = pd.read_csv(path + 'ytest_pred.csv')\n",
    "\n",
    "X = train_x.loc[:,~train_x.columns.isin(['type'])].copy()\n",
    "y = train_x['type'].apply(lambda x: 1 if x=='ELECTRICAL' else 0) \n",
    "X_test = test_x.copy()\n",
    "y_test = test_y\n",
    "\n",
    "X = data_preprocessing(X)\n",
    "X_test = data_preprocessing(X_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Train - Test data prep: ', stop - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "\n",
    "def machine_learning_prep(train_X, train_y, test_X, test_y):\n",
    "    \n",
    "    # Prep training data\n",
    "    X = train_X.values\n",
    "    y = train_y.values\n",
    "    X_test = test_X.iloc[:25148,:].values\n",
    "    y_test = test_y.values\n",
    "    \n",
    "    # Train Validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                      test_size=0.25, \n",
    "                                                      random_state=1, \n",
    "                                                      stratify=y)\n",
    "    \n",
    "    # Transform licensetype, businessname and subtype using OneHotEncoding\n",
    "    column_trans = make_column_transformer((OneHotEncoder(sparse=False, handle_unknown='ignore'), [0, 1, 3]),\n",
    "                                           remainder='passthrough')\n",
    "    \n",
    "    X_train = column_trans.fit_transform(X_train)\n",
    "    X_val = column_trans.transform(X_val)\n",
    "    X_test = column_trans.transform(X_test)\n",
    "    \n",
    "    \n",
    "    # Transform Description using tf-idf\n",
    "    tf = TfidfVectorizer(min_df=1, stop_words='english', lowercase=False)\n",
    "    X_train[:,-3] = tf.fit_transform(X_train[:,-3]).toarray().sum(axis=1)\n",
    "    X_val[:,-3] = tf.transform(X_val[:,-3]).toarray().sum(axis=1)\n",
    "    X_test[:,-3] = tf.transform(X_test[:,-3]).toarray().sum(axis=1)\n",
    "    \n",
    "    # Training model with XGBoost\n",
    "    classifier = XGBClassifier(use_label_encoder=False)\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluating Model\n",
    "    y_pred = classifier.predict(X_val)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    # Evaluating model on test\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    return cm, accuracy, cm_test, accuracy_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Accuracy\n",
      "[[17429   448]\n",
      " [  687  6475]] \n",
      " 0.9546707136866488 \n",
      "\n",
      "Test Accuracy\n",
      "[[14142  4076]\n",
      " [ 4091  2839]] \n",
      " 0.6752425640209957\n",
      "\n",
      "\n",
      "Baseline Model Training:  0.0008582729933550581\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "cm, accuracy, cm_test, accuracy_test = machine_learning_prep(X, y, X_test, y_test)\n",
    "print('\\n')\n",
    "print('Training Accuracy')\n",
    "print(cm, '\\n', accuracy, '\\n')\n",
    "print('Test Accuracy')\n",
    "print(cm_test, '\\n', accuracy_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('\\n')\n",
    "print('Baseline Model Training: ', stop - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
